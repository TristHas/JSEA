\begin{thebibliography}{10}

\bibitem{jacob2018quantization}
Benoit Jacob, Skirmantas Kligys, Bo~Chen, Menglong Zhu, Matthew Tang, Andrew
  Howard, Hartwig Adam, and Dmitry Kalenichenko.
\newblock Quantization and training of neural networks for efficient
  integer-arithmetic-only inference.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 2704--2713, 2018.

\bibitem{molchanov2016pruning}
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock {\em arXiv preprint arXiv:1611.06440}, 2016.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{shallue2018measuring}
Christopher~J Shallue, Jaehoon Lee, Joe Antognini, Jascha Sohl-Dickstein, Roy
  Frostig, and George~E Dahl.
\newblock Measuring the effects of data parallelism on neural network training.
\newblock {\em arXiv preprint arXiv:1811.03600}, 2018.

\bibitem{mccandlish2018empirical}
Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI~Dota Team.
\newblock An empirical model of large-batch training.
\newblock {\em arXiv preprint arXiv:1812.06162}, 2018.

\bibitem{you2017large}
Yang You, Igor Gitman, and Boris Ginsburg.
\newblock Large batch training of convolutional networks.
\newblock {\em arXiv preprint arXiv:1708.03888}, 2017.

\bibitem{iandola2016squeezenet}
Forrest~N Iandola, Song Han, Matthew~W Moskewicz, Khalid Ashraf, William~J
  Dally, and Kurt Keutzer.
\newblock Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5
  mb model size.
\newblock {\em arXiv preprint arXiv:1602.07360}, 2016.

\bibitem{howard2017mobilenets}
Andrew~G Howard, Menglong Zhu, Bo~Chen, Dmitry Kalenichenko, Weijun Wang,
  Tobias Weyand, Marco Andreetto, and Hartwig Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock {\em arXiv preprint arXiv:1704.04861}, 2017.

\bibitem{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock {\em arXiv preprint arXiv:1803.03635}, 2018.

\bibitem{micikevicius2017mixed}
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen,
  David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh
  Venkatesh, et~al.
\newblock Mixed precision training.
\newblock {\em arXiv preprint arXiv:1710.03740}, 2017.

\bibitem{wu2018training}
Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi.
\newblock Training and inference with integers in deep neural networks.
\newblock {\em arXiv preprint arXiv:1802.04680}, 2018.

\bibitem{martens2012training}
James Martens and Ilya Sutskever.
\newblock Training deep and recurrent networks with hessian-free optimization.
\newblock In {\em Neural networks: Tricks of the trade}, pages 479--535.
  Springer, 2012.

\bibitem{chen2016training}
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
\newblock Training deep nets with sublinear memory cost.
\newblock {\em arXiv preprint arXiv:1604.06174}, 2016.

\bibitem{gomez2017reversible}
Aidan~N Gomez, Mengye Ren, Raquel Urtasun, and Roger~B Grosse.
\newblock The reversible residual network: Backpropagation without storing
  activations.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2214--2224, 2017.

\bibitem{jacobsen2018revnet}
J{\"o}rn-Henrik Jacobsen, Arnold Smeulders, and Edouard Oyallon.
\newblock i-revnet: Deep invertible networks.
\newblock {\em arXiv preprint arXiv:1802.07088}, 2018.

\bibitem{rota2018place}
Samuel Rota~Bul{\`o}, Lorenzo Porzi, and Peter Kontschieder.
\newblock In-place activated batchnorm for memory-optimized training of dnns.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 5639--5647, 2018.

\bibitem{huang2017densely}
Gao Huang, Zhuang Liu, Laurens Van Der~Maaten, and Kilian~Q Weinberger.
\newblock Densely connected convolutional networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4700--4708, 2017.

\bibitem{ronneberger2015u}
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In {\em International Conference on Medical image computing and
  computer-assisted intervention}, pages 234--241. Springer, 2015.

\bibitem{paszke2017automatic}
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
  DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
\newblock Automatic differentiation in {PyTorch}.
\newblock In {\em NIPS Autodiff Workshop}, 2017.

\bibitem{wu2018shift}
Bichen Wu, Alvin Wan, Xiangyu Yue, Peter Jin, Sicheng Zhao, Noah Golmant, Amir
  Gholaminejad, Joseph Gonzalez, and Kurt Keutzer.
\newblock Shift: A zero flop, zero parameter alternative to spatial
  convolutions.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 9127--9135, 2018.

\end{thebibliography}
